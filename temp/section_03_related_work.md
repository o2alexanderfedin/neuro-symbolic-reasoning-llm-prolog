# 3. Related Work

Our approach bridges multiple research areas: neuro-symbolic AI, formal verification with SMT solvers, explainable AI, and logic programming. In this section, we position our work within the broader landscape of these fields, highlighting how our LLM-generated Prolog predicates with embedded reasoning differ fundamentally from existing approaches.

## 3.1 Neuro-Symbolic AI

The neuro-symbolic AI paradigm seeks to combine the learning capabilities of neural networks with the reasoning capabilities of symbolic systems [Garcez et al. 2022]. Recent work has demonstrated impressive results by integrating these two complementary approaches.

**Neural Module Networks** [Andreas et al. 2016] pioneered compositional neural architectures where questions are parsed into linguistic substructures that dynamically instantiate modular networks. Each module performs specific operations (e.g., "find," "relate," "count") that compose into deeper networks for visual question answering. While innovative, NMNs require hand-crafted module libraries with predefined functionality. Our approach differs fundamentally: rather than selecting from fixed symbolic components, we generate domain-specific predicates on-demand for each reasoning task, providing greater flexibility across arbitrary domains.

The **Neuro-Symbolic Concept Learner** [Mao et al. 2019] extends this idea by learning visual concepts, words, and semantic parsing from natural supervision. NS-CL builds object-based scene representations and translates questions into executable symbolic programs. However, NS-CL remains confined to the visual reasoning domain (primarily CLEVR-like tasks) with weak semantic parsing that lacks formal grounding. Our framework generalizes beyond visual reasoning to handle arbitrary domains—temporal, medical, legal, business—without domain-specific engineering.

Recent **neural theorem provers** represent another prominent thread in neuro-symbolic AI. GPT-f [Polu & Sutskever 2020] applies transformer language models to automated theorem proving in Metamath, achieving 56.22% pass rate and contributing the first deep learning-generated proofs accepted by a formal mathematics community. LeanDojo [Yang et al. 2023] advances this with retrieval-augmented language models for Lean theorem proving, constructing benchmarks of 98,734 theorems. DeepSeek-Prover [Shao et al. 2024] addresses data scarcity through large-scale synthetic data generation (8 million Lean 4 formal statements), while AlphaGeometry [Trinh et al. 2024] solves olympiad geometry problems by synthesizing millions of theorems without human demonstrations.

These neural theorem provers demonstrate the power of combining neural generation with formal verification. However, they focus narrowly on mathematical theorem proving within single proof systems (Metamath, Lean, Isabelle). Our approach differs in scope and purpose: we target practical reasoning tasks across diverse domains rather than formal mathematics. More critically, while theorem provers generate formal proofs in mathematical calculi, our predicates produce **semantic proof certificates in natural domain language**. A theorem prover might output a proof in the Lean calculus; our system outputs reasoning like "The deployment cannot happen before the build completes because the deployment requires the build artifact." This semantic transparency makes our explanations accessible to domain experts without formal methods training.

**Scallop** [Li et al. 2023] represents a different approach: a Datalog-based language supporting differentiable logical and relational reasoning through provenance semirings. Scallop integrates with PyTorch for end-to-end training, maintaining differentiability while supporting recursion, aggregation, and negation. However, Scallop requires manual program specification by users with expertise in Datalog and logic programming. Our approach inverts this: the LLM generates the logic programs automatically from natural language descriptions, eliminating the need for users to write formal specifications.

**Key Distinction**: Existing neuro-symbolic systems operate with fixed symbolic components—whether predefined modules (NMNs), domain-specific visual reasoning (NS-CL), mathematical proof systems (GPT-f, LeanDojo), or manually specified Datalog programs (Scallop). Our architecture generates the symbolic layer per-task, creating domain-appropriate predicates with embedded reasoning on-demand. This flexibility enables handling arbitrary domains without extensive pre-engineering while maintaining the rigor of symbolic reasoning.

## 3.2 SMT-Based Verification

Satisfiability Modulo Theories (SMT) solvers represent the state-of-the-art in formal verification and constraint solving. Modern solvers like Z3 [de Moura & Bjørner 2008], cvc5 [Barbosa et al. 2022], and veriT [Bouton et al. 2009] combine SAT solving with specialized decision procedures for theories including linear arithmetic, arrays, bit-vectors, and uninterpreted functions.

**Temporal Reasoning with SMT**: The application of SMT to temporal reasoning builds on Allen's Interval Algebra [Allen 1983], which defines thirteen basic qualitative relations between time intervals (before, after, meets, overlaps, during, etc.). Simple Temporal Networks [Dechter et al. 1991] extended this framework to metric temporal constraints, representing time points with unary and binary constraints on time differences.

Cimatti, Micheli, and Roveri pioneered SMT-based approaches to temporal problems with uncertainty, addressing strong controllability [Cimatti et al. 2012, 2015] and weak controllability [Cimatti et al. 2015] for temporal constraint networks. Their work demonstrates that SMT solvers can effectively handle temporal uncertainty by encoding problems in Linear Real Arithmetic with quantifiers, using quantifier elimination to produce efficient quantifier-free formulas.

**Proof Production in SMT**: Modern SMT solvers produce independently verifiable proof certificates. The **Alethe proof format** [Schurr et al. 2021] evolved from veriT's format and is now generated by both veriT and cvc5, with reconstruction support in Isabelle/HOL and Coq (via SMTCoq). Alethe provides SMT-LIB-compatible syntax supporting both coarse-grained and fine-grained proof steps. The **LFSC framework** [Stump et al. 2013] offers an alternative based on the Edinburgh Logical Framework extended with side conditions, allowing flexible proof system descriptions. **Carcara** [Andreotti et al. 2023], an efficient Alethe proof checker implemented in Rust, provides push-button verification and proof elaboration, transforming coarse-grained steps into fine-grained ones for proof assistant integration.

These proof formats serve a critical purpose: providing formal, machine-checkable certificates that solver results are correct. A proof in Alethe or LFSC can be independently verified by separate checkers, establishing trustworthiness without trusting the solver implementation.

**Key Distinctions from Our Approach**:

1. **Proof Language vs. Domain Language**: SMT proofs are expressed in formal calculi (Alethe, LFSC) designed for machine verification. These proofs reference constraints by identifiers and encode inference steps in terms of logical rules from formal systems. Our predicates produce reasoning in the natural language of the problem domain. Instead of "UNSAT core: [c1, c3, c7]," our system explains: "The deployment cannot happen before the build completes (10-minute lag required) because the deployment requires the build artifact, and the incident was caused by the deployment."

2. **Fixed Theories vs. Domain-Specific Predicates**: SMT solvers work with pre-defined theories—QF_LIA (quantifier-free linear integer arithmetic), QF_UF (uninterpreted functions), arrays, bit-vectors. To use an SMT solver, one must encode the problem in these theories. For temporal reasoning, this means encoding events as integer variables representing time points, writing constraints in arithmetic, and interpreting results. Our approach generates domain-specific predicates on-demand: `happens_before/3`, `suggests_flu/3`, `contract_valid/2`. These predicates operate directly in domain concepts rather than requiring translation to/from arithmetic encodings.

3. **Formal Verification vs. Semantic Verification**: SMT produces formal proofs verifiable by proof assistants like Isabelle/HOL or Coq. These proofs provide mathematical guarantees of correctness based on formal logics. Our approach produces semantic proofs—reasoning chains expressed in domain terminology that domain experts can validate. We trade formal mathematical guarantees for domain-level transparency and auditability. For safety-critical systems requiring mathematical certification, SMT is superior. For applications requiring human understanding and domain expert validation (medical diagnosis, business rules, legal reasoning), semantic proofs in domain language are more appropriate.

4. **Two-Stage Process**: Using SMT for reasoning from natural language requires two distinct phases: (1) LLM translates natural language to SMT-LIB encoding, (2) SMT solver produces formal proof. Our approach unifies these: the LLM generates Prolog predicates that simultaneously encode the logic AND embed the reasoning justifications. Execution produces both the answer and the domain-language explanation in a single pass.

**Complementary Strengths**: We view our approach as complementary to SMT rather than competing. SMT excels at formal verification with mathematical guarantees—essential for safety-critical systems, hardware verification, and regulatory compliance requiring proof assistant integration. Our approach excels at explainable reasoning in natural domain language—essential for interactive applications, multi-domain reasoning, and scenarios where domain experts (not formal methods experts) must validate decisions. A hybrid architecture using both could provide formal verification certificates (via SMT) alongside human-readable explanations (via our approach).

## 3.3 Explainable AI

The explainable AI (XAI) field addresses the black-box nature of modern machine learning models through various explanation techniques. However, a critical distinction exists between post-hoc explanations and intrinsic interpretability [Springer 2025; Molnar 2022].

**Post-Hoc Explanation Methods**: The dominant XAI paradigm develops techniques to explain opaque models after training. LIME (Local Interpretable Model-agnostic Explanations) [Ribeiro et al. 2016] approximates complex models locally with interpretable linear models, generating explanations by perturbing inputs and observing prediction changes. SHAP (SHapley Additive exPlanations) [Lundberg & Lee 2017] applies Shapley values from cooperative game theory to provide consistent, locally accurate feature attributions.

Gradient-based methods offer alternative post-hoc approaches. Saliency maps [Simonyan et al. 2014] visualize which pixels most affect predictions through gradients, while Grad-CAM [Selvaraju et al. 2017] provides class-discriminative localization for CNNs. Integrated Gradients [Sundararajan et al. 2017] satisfies axiomatic properties (sensitivity and implementation invariance) through path integrals of gradients, and Layer-wise Relevance Propagation [Bach et al. 2015] propagates relevance backward through networks.

Counterfactual explanations [Wachter et al. 2018] answer "what if" questions by finding minimal input perturbations that change predictions, addressing GDPR's "right to explanation" requirements. While useful for understanding model sensitivity, counterfactuals only show alternative scenarios without revealing underlying decision logic.

**Attention Mechanisms**: Transformers [Vaswani et al. 2017] and earlier attention mechanisms [Bahdanau et al. 2015] are sometimes claimed to provide interpretability through attention weights. However, Jain and Wallace [2019] demonstrate that attention weights often fail to provide reliable explanations—they are uncorrelated with gradient-based importance measures, and multiple attention distributions can yield identical predictions. Attention provides a mechanism for information flow but not necessarily a faithful explanation of model decisions.

**Fundamental Limitation of Post-Hoc Methods**: Rudin [2019] argues forcefully that post-hoc explanations of black-box models are fundamentally flawed for high-stakes decisions. Post-hoc explanations create separate models (LIME's local approximations, SHAP's additive explanations) that may not faithfully represent the original model's decision process. This introduces additional complexity and potential for misleading explanations. Rudin advocates for inherently interpretable models instead—models whose decision-making process is transparent by design.

**Intrinsic Interpretability**: Generalized Additive Models (GAMs) [Hastie & Tibshirani 1986] exemplify interpretable-by-design approaches, using additive structure where each feature's contribution is separately visualizable. Similarly, decision trees and rule-based systems provide inherent interpretability through their structural transparency.

**Key Distinction from Our Approach**: All post-hoc explanation methods—LIME, SHAP, gradients, attention, counterfactuals—generate explanations as a separate step after model training and prediction. They approximate or analyze opaque models to extract explanations. Our predicates embed reasoning intrinsically: the reasoning parameters (`Reasoning`, `Justification`, `Authorization`, `Proof`) are integral to the predicate definitions, not post-hoc additions. Execution simultaneously computes the answer AND generates the explanation. The explanation is not an approximation or analysis of the decision process—it IS the decision process made explicit.

This intrinsic explainability provides several advantages:

1. **Guaranteed Fidelity**: The explanation exactly reflects the reasoning because they are inseparable. There is no separate explanation model that might misrepresent the original.

2. **No Additional Complexity**: Post-hoc methods add layers of complexity (LIME's local models, SHAP's coalitional game computations). Our explanations emerge naturally from predicate execution.

3. **Verifiable**: The reasoning can be inspected, debugged, and verified using standard logic programming tools. Domain experts can validate that the rules correctly encode domain knowledge.

4. **Counterfactuals via Logical Inference**: Rather than computing counterfactuals through optimization, we derive them through logical reasoning. The `cannot_perform_action/3` predicate explains what would need to change: `would_succeed_if: has_capability(Actor, RequiredCap)`.

**Neural Proof Generation**: Recent work on neural theorem provers (discussed in Section 3.1) represents another form of XAI where neural networks generate formal proofs. While these proofs are verifiable and transparent, they are expressed in formal calculi (Lean, Metamath) requiring expertise to interpret. Our semantic proofs in domain language bridge the gap between formal verification and human understanding.

## 3.4 Logic Programming for AI

Logic programming provides a declarative paradigm where programs specify what to compute rather than how to compute it, offering inherent transparency and formal semantics.

**Answer Set Programming (ASP)**: ASP [Gelfond & Lifschitz 1988, 1991] extends logic programming with stable model semantics, enabling declarative specification of search and optimization problems. Modern ASP solvers like Clingo [Gebser et al. 2011, 2019] use conflict-driven clause learning and support multi-shot solving for incremental and reactive reasoning. ASP has been successfully applied to planning, scheduling, configuration, and knowledge representation.

Critically, ASP supports explanation generation. Pontelli et al. [2009] developed formal foundations for justifications in ASP, providing graph-based explanations for why atoms appear in answer sets. Fandinno and Schulz [2019] survey explanation approaches for ASP, addressing "why" and "why-not" queries through derivation trees and proof structures. Systems like xClingo [Buford & Jakobovits 2020] provide interactive explanation interfaces with natural language generation from annotated ASP programs.

**Constraint Logic Programming (CLP)**: CLP [Jaffar & Lassez 1987; Jaffar & Maher 1994] combines logic programming with constraint solving over various domains (real numbers, finite domains, bit-vectors). CLP(FD) [Triska 2012] provides powerful constraint solving over finite domains with propagation, reification, and search strategies. The declarative nature of CLP makes constraint specifications explicit and inspectable—every constraint and its propagation can be traced and explained.

**Inductive Logic Programming (ILP)**: ILP [Muggleton 1991] learns first-order logic programs from examples, representing the intersection of machine learning and logic programming. Unlike black-box ML approaches, ILP produces human-readable logic programs as learned hypotheses. Recent work [Cropper et al. 2022; Kaminski et al. 2023] positions ILP as a natural approach for explainable AI since learned rules are inherently interpretable. ILP demonstrates that learning and interpretability are not mutually exclusive.

**Declarative Semantics and Explainability**: The foundational semantics of logic programming [Van Emden & Kowalski 1976; Kowalski 1979] established that logic programs have both declarative (what) and procedural (how) interpretations. Robinson's resolution principle [Robinson 1965] provides the computational mechanism, where resolution proof trees explicitly represent inference steps. This duality enables explanation at multiple levels: we can explain what a program computes (declarative semantics) and how it computes it (operational semantics with proof traces).

**Key Distinction from Our Approach**: Traditional logic programming requires hand-crafted rules. Domain experts or knowledge engineers must manually encode domain knowledge as logic programs—a labor-intensive process requiring expertise in both the domain and logic programming. ASP applications typically involve weeks or months of knowledge acquisition and rule refinement. CLP requires formulating problems as constraint satisfaction problems with appropriate constraint postings. ILP can learn rules from data but requires carefully curated examples and often produces rules that need expert validation and refinement.

Our approach automates rule generation: the LLM translates natural language problem descriptions into domain-specific Prolog predicates. This eliminates the manual knowledge engineering bottleneck. A user can describe a medical diagnosis scenario, business approval workflow, or temporal reasoning problem in natural language, and the LLM generates appropriate predicates with embedded reasoning. While the generated rules should be validated (as with any automatically generated code), this is faster than writing rules from scratch and accessible to domain experts without programming expertise.

**RAG-Based Template Libraries**: Our approach combines generation flexibility with reusability. Initial queries generate templates (1.5-2 seconds) that are cached in vector databases for semantic retrieval. Subsequent similar queries retrieve templates (10-50ms), extract data from the query (50-200ms), and instantiate templates with specific data (&lt;5ms)—achieving 20-60x speedup over regeneration while maintaining the ability to generate novel templates for new domains. This creates an evolving template library similar to traditional logic programming knowledge bases but with automatic construction through LLM generation and semantic retrieval.

**Complementary Strengths**: Our approach complements traditional logic programming. For well-established domains with stable rule sets (e.g., tax regulations, formal standards), hand-crafted ASP/CLP programs validated by experts remain appropriate. For rapidly evolving domains, exploratory analysis, and scenarios requiring quick adaptation to new problem types, LLM-generated predicates provide agility. A hybrid approach might use manually crafted rules for critical invariants and LLM-generated rules for flexible reasoning, or use ILP to refine LLM-generated rules based on data.

---

**Summary**: Our approach occupies a unique position in the research landscape. From neuro-symbolic AI, we adopt the integration of neural and symbolic methods but generate symbolic components per-task rather than using fixed architectures. From SMT-based verification, we draw inspiration for rigorous constraint solving but produce semantic proofs in domain language rather than formal calculi. From explainable AI, we embrace the goal of transparency but achieve it through intrinsic interpretability rather than post-hoc explanations. From logic programming, we leverage declarative semantics and formal reasoning but automate rule generation through LLMs rather than requiring manual knowledge engineering.

This synthesis enables a new paradigm: **reasoning-as-code with semantic proof certificates**. The LLM generates self-documenting predicates that carry their own explanations, bridging the gap between formal rigor and human understanding, between fixed ontologies and flexible generation, between opaque neural reasoning and transparent symbolic logic.
